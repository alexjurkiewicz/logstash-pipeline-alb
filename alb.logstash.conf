input {
    s3 {
        bucket => "${BUCKET}"
        add_field => {
            "[file][s3][bucket]" => "${BUCKET}"
        }
        exclude_pattern => "\A(.*/)?ELBAccessLogTestFile\Z"
        # For some reason this isn't auto-detected from the environment, so we
        # set it explicitly to prevent "multiple unnecessary redirects and
        # signing attempts" (though really, why would logstash need to be told
        # more than once about the correct region??).
        region => "${REGION}"
        # Set when testing locally
        # access_key_id => "123"
        # secret_access_key => "123"
    }
}

filter {
    grok {
        id => "Parse access log lines into fields"
        patterns_dir => [ "/usr/share/logstash/config/alb-patterns" ]
        match => {
            message => "%{ALB_ACCESS_LOG}"
        }
    }
    date {
        id => "Parse event.end timestamp field"
        match => [ "[event][end]", "ISO8601"]
        target => "[event][end]"
    }
    date {
        id => "Parse event.start timestamp field"
        match => [ "[event][start]", "ISO8601"]
        target => "[event][start]"
    }
    ruby {
        # This is needed because the S3 input plugin restarts processing every time Logstash starts up
        id => "Drop old log events"
        init => "require 'time'"
        code => "event.cancel if event.timestamp.to_i < (Time.now.to_i - (86400 * ${MAX_INGEST_AGE_DAYS}))"
    }
    ruby {
        id => "Convert fields with float seconds to integer milliseconds"
        code => "%w([http][lb][request_processing_time] [http][lb][target_processing_time] [http][lb][response_processing_time]).each { |field| event.set(field, event.get(field).to_i * 1000) if event.get(field) }"
    }
    geoip {
        id => "Add geo data"
        source => "[client][ip]"
        fields => ["city_name", "country_name", "region_name", "continent_code", "latitude", "longitude"]
        target => "geo"
    }
    mutate {
        id => "Clean up geo data"
        rename => {
            "[geo][latitude]" => "[geo][location][lat]"
            "[geo][longitude]" => "[geo][location][lon]"
        }
    }
    # Add user agent data. Needs a bit of cleanup to preserve original string.
    useragent {
        id => "Add user agent detail"
        source => "user_agent_original"
        target => "user_agent"
    }
    mutate {
        id => "Fix up user_agent.original"
        rename => {
            "user_agent_original" => "[user_agent][original]"
        }
    }
    mutate {
        id => "Cleanup"
        remove_field => [ "@version", "host" ]
        rename => {
            "message" => "[log][original]"
        }
        copy => {
            "[@metadata][s3][key]" => "[file][s3][key]"
            "[event][end]" => "@timestamp"
        }
        add_field => {
            "[ecs][version]" => "1.0.1"
        }
        split => {
            "[http][lb][actions]" => ","
        }
        remove_tag => [ "_geoip_lookup_failure" ]
    }
    fingerprint {
        id => "Set a document ID"
        concatenate_all_fields => true
        target => "[metadata][document_id]"
        key => "ALB parsing"
    }
}

output {
    elasticsearch {
        hosts => ["${ES_HOST}"]
        # Use our custom document ID and ignore duplicate ID failure messages
        document_id => "%{[@metadata][document_id]}"
        failure_type_logging_whitelist => [409]
    }
}
